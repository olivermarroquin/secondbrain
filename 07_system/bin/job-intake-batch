#!/Users/olivermarroquin/secondbrain/07_system/venvs/docgen/bin/python
import argparse
import sys
import os
from pathlib import Path as _Path

# add resume-factory/scripts to sys.path (hyphen-safe)
_ROOT = _Path(os.path.expanduser("~/secondbrain"))
_RF_SCRIPTS = _ROOT / "01_projects" / "resume-factory" / "scripts"
sys.path.insert(0, str(_RF_SCRIPTS))

from rf_job_intake_batch_core import read_urls_file, scrape_job_page, slugify_snake, slugify_kebab, validate_parsed_job
from rf_job_folder_writer import JobIntake, write_job_folder, today_yyyy_mm_dd


def die(msg, code=2):
    print(f"ERROR: {msg}", file=sys.stderr)
    sys.exit(code)





def read_stdin_urls(limit: int):
    if sys.stdin.isatty():
        return None
    urls = []
    for line in sys.stdin.read().splitlines():
        line = line.strip()
        if not line:
            continue
        urls.append(line)
        if len(urls) >= limit:
            break
    return urls

def collect_interactive_urls(limit: int):
    print(f"Enter up to {limit} job URLs (one per line).")
    print("Press ENTER on empty line to stop.")
    urls = []
    while len(urls) < limit:
        try:
            u = input("> ").strip()
        except EOFError:
            break
        if not u:
            break
        urls.append(u)
    return urls

def main():
    ap = argparse.ArgumentParser(prog="job-intake-batch")

    ap.add_argument("limit", nargs="?", type=int, help="Number of jobs to intake (interactive mode)")
    ap.add_argument("--urls-file", help="Path to file containing URLs (one per line)")
    ap.add_argument("--family", default="qa_automation_engineer")
    ap.add_argument("--ai-parse", action="store_true")
    ap.add_argument("--dry-run", action="store_true")
    ap.add_argument("--force", action="store_true")
    ap.add_argument("--insecure", action="store_true", help="Disable SSL verification (use only if your cert store is broken)")

    args = ap.parse_args()

    if not args.urls_file and not args.limit:
        die("Must provide either <N> or --urls-file")

    if args.urls_file:
        urls = read_urls_file(args.urls_file)
        print(f"MODE: urls-file")
        print(f"COUNT: {len(urls)}")

        created_count = 0
        skipped_count = 0
        failed_count = 0

        for i, u in enumerate(urls, start=1):
            print(f"--- [{i}/{len(urls)}] {u}")
            try:
                data = scrape_job_page(u, verify_ssl=not args.insecure, ai_parse=args.ai_parse)
            except Exception as e:
                failed_count += 1
                print(f"FAILED: scrape error: {e}", file=sys.stderr)
                continue

            print(f"PAGE_TITLE: {data.get('page_title')}")
            if args.ai_parse:
                print(f"AI_PARSED: {data.get('ai_parsed')}")
                if data.get("ai_parse_error"):
                    print(f"AI_PARSE_ERROR: {data.get('ai_parse_error')}")
            print(f"PROV_COMPANY: {data.get('company')}")
            print(f"PROV_ROLE: {data.get('role_title')}")
            desc = (data.get("description") or "").splitlines()
            print(f"DESC_LINES: {len(desc)}")
            if desc:
                print("DESC_HEAD:")
                for ln in desc[:5]:
                    print(ln)
                print("DESC_TAIL:")
                for ln in desc[-2:]:
                    print(ln)

            # Build deterministic intake payload (v1: provisional fields)
            company_raw = data.get("company") or "unknown"
            role_raw = data.get("role_title") or "unknown"

            company_slug = slugify_snake(company_raw) or "unknown"
            role_title = role_raw.strip() if isinstance(role_raw, str) else "unknown"
            role_slug = slugify_kebab(role_title) or "unknown"
            ok, reason = validate_parsed_job(company_raw, role_title, "\n".join(desc))
            if not ok:
                failed_count += 1
                print(f"FAILED: validation_error: {reason}", file=sys.stderr)
                continue

            ji = JobIntake(
                family=args.family,
                company_slug=company_slug,
                date_found=today_yyyy_mm_dd(),
                role_slug=role_slug,
                role_title=role_title,
                source_url=u,
                location=data.get("location") or "unknown",
            )

            status, app_dir, detail = write_job_folder(
                root=_ROOT,
                ji=ji,
                jd_text="\n".join(desc),
                dry_run=args.dry_run,
                force=args.force,
            )
            print(f"RESULT: {status}  PATH: {app_dir}  DETAIL: {detail}")
            if status == "CREATED":
                created_count += 1
            elif status == "SKIPPED":
                skipped_count += 1
            elif status == "FAILED":
                failed_count += 1
        print()
        print("SUMMARY")
        print(f"CREATED: {created_count}")
        print(f"SKIPPED: {skipped_count}")
        print(f"FAILED : {failed_count}")

        if failed_count > 0:
            sys.exit(1)
        return

    # interactive mode
    urls = read_stdin_urls(args.limit) or collect_interactive_urls(args.limit)
    if not urls:
        die("No URLs provided.")

    print(f"MODE: interactive")
    print(f"COUNT: {len(urls)}")

    created_count = 0
    skipped_count = 0
    failed_count = 0

    for i, u in enumerate(urls, start=1):
        print(f"--- [{i}/{len(urls)}] {u}")
        try:
            data = scrape_job_page(u, verify_ssl=not args.insecure, ai_parse=args.ai_parse)
        except Exception as e:
            failed_count += 1
            print(f"FAILED: scrape error: {e}", file=sys.stderr)
            continue

        desc = (data.get("description") or "").splitlines()

        company_raw = data.get("company") or "unknown"
        role_raw = data.get("role_title") or "unknown"

        company_slug = slugify_snake(company_raw) or "unknown"
        role_title = role_raw.strip() if isinstance(role_raw, str) else "unknown"
        role_slug = slugify_kebab(role_title) or "unknown"
        ok, reason = validate_parsed_job(company_raw, role_title, "\n".join(desc))
        if not ok:
            failed_count += 1
            print(f"FAILED: validation_error: {reason}", file=sys.stderr)
            continue

        ji = JobIntake(
            family=args.family,
            company_slug=company_slug,
            date_found=today_yyyy_mm_dd(),
            role_slug=role_slug,
            role_title=role_title,
            source_url=u,
            location=data.get("location") or "unknown",
        )

        status, app_dir, detail = write_job_folder(
            root=_ROOT,
            ji=ji,
            jd_text="\n".join(desc),
            dry_run=args.dry_run,
            force=args.force,
        )

        print(f"RESULT: {status}  PATH: {app_dir}  DETAIL: {detail}")

        if status == "CREATED":
            created_count += 1
        elif status == "SKIPPED":
            skipped_count += 1
        elif status == "FAILED":
            failed_count += 1

    print()
    print("SUMMARY")
    print(f"CREATED: {created_count}")
    print(f"SKIPPED: {skipped_count}")
    print(f"FAILED : {failed_count}")

    if failed_count > 0:
        sys.exit(1)

    return

if __name__ == "__main__":
    main()
